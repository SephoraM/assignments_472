----------------------------------------------------------------------------------------------------
(A) Model: Base DT - Default parameters

(B) Confusion Matrix:
          Predicted 0  Predicted 1  Predicted 2
Actual 0           42            1            0
Actual 1            1           20            0
Actual 2            0            0           20

(C) Precision, Recall, and F1-Measure for Each Class:
   Precision    Recall  F1-Score  Support
0   0.976744  0.976744  0.976744       43
1   0.952381  0.952381  0.952381       21
2   1.000000  1.000000  1.000000       20

(D) Accuracy: 0.9762
Macro-average F1: 0.9764
Weighted-average F1: 0.9762

----------------------------------------------------------------------------------------------------
(A) Model: Top DT - Best hyperparameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 6}

(B) Confusion Matrix:
          Predicted 0  Predicted 1  Predicted 2
Actual 0           42            1            0
Actual 1            5           16            0
Actual 2            1            0           19

(C) Precision, Recall, and F1-Measure for Each Class:
   Precision    Recall  F1-Score  Support
0   0.875000  0.976744  0.923077       43
1   0.941176  0.761905  0.842105       21
2   1.000000  0.950000  0.974359       20

(D) Accuracy: 0.9167
Macro-average F1: 0.9132
Weighted-average F1: 0.9150

----------------------------------------------------------------------------------------------------
(A) Model: Base MLP - hidden_layer_sizes=(100,100), activation=logistic, solver=sgd

(B) Confusion Matrix:
          Predicted 0  Predicted 1  Predicted 2
Actual 0           43            0            0
Actual 1           21            0            0
Actual 2           20            0            0

(C) Precision, Recall, and F1-Measure for Each Class:
   Precision  Recall  F1-Score  Support
0   0.511905     1.0  0.677165       43
1   0.000000     0.0  0.000000       21
2   0.000000     0.0  0.000000       20

(D) Accuracy: 0.5119
Macro-average F1: 0.2257
Weighted-average F1: 0.3466

----------------------------------------------------------------------------------------------------
(A) Model: Top MLP - Best hyperparameters: {'activation': 'logistic', 'hidden_layer_sizes': (20, 10), 'solver': 'adam'}

(B) Confusion Matrix:
          Predicted 0  Predicted 1  Predicted 2
Actual 0           43            0            0
Actual 1           21            0            0
Actual 2            2            0           18

(C) Precision, Recall, and F1-Measure for Each Class:
   Precision  Recall  F1-Score  Support
0   0.651515     1.0  0.788991       43
1   0.000000     0.0  0.000000       21
2   1.000000     0.9  0.947368       20

(D) Accuracy: 0.7262
Macro-average F1: 0.5788
Weighted-average F1: 0.6295

-------- Running all models 5 times leads to the following averages: 

(A) Average accuracy:0.9714285714285713 
(B) Average macro:0.9714593167680577 
(C) Average weighted:0.971449623173761 
-------- Running all models 5 times leads to the following averages: 

(A) Average accuracy:0.9452380952380952 
(B) Average macro:0.9423310231633366 
(C) Average weighted:0.9446410329064638 
-------- Running all models 5 times leads to the following averages: 

(A) Average accuracy:0.45714285714285713 
(B) Average macro:0.20621845346254797 
(C) Average weighted:0.2956303538980704 
-------- Running all models 5 times leads to the following averages: 

(A) Average accuracy:0.5833333333333333 
(B) Average macro:0.3515484954399696 
(C) Average weighted:0.44512721600374067 
